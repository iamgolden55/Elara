#!/usr/bin/env python3
"""
üß™ Test script for model loading - verify DialoGPT + LoRA integration
"""

import torch
import os
import sys
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import time

# Add the project root to the Python path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# Import the settings from our config
from backend.config import Settings

def test_model_loading():
    """Test loading the medical LoRA model"""
    
    print("üß™ Testing model loading...")
    settings = Settings()
    
    # Get the medical_qa model config
    model_config = settings.get_model_config("medical_qa")
    lora_path = model_config["path"]
    
    print(f"üìÅ Looking for LoRA adapter at: {lora_path}")
    
    if not lora_path.exists():
        print(f"‚ùå Error: LoRA path doesn't exist: {lora_path}")
        return False
    
    try:
        # Load the PEFT configuration
        print("üìÑ Loading PEFT configuration...")
        peft_config = PeftConfig.from_pretrained(str(lora_path))
        print(f"‚úÖ Found config. Base model: {peft_config.base_model_name_or_path}")
        
        # Load base model and tokenizer
        print("ü§ñ Loading base model. This may take a moment...")
        start_time = time.time()
        
        base_model = AutoModelForCausalLM.from_pretrained(
            peft_config.base_model_name_or_path,
            torch_dtype=torch.float32,    # Use float32 for CPU compatibility
            low_cpu_mem_usage=True        # Optimize memory usage
        )
        
        print(f"‚è±Ô∏è Base model loaded in {time.time() - start_time:.2f} seconds")
        
        tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)
        print("üî§ Tokenizer loaded")
        
        # Load the LoRA adapter
        print("üß† Loading LoRA adapter...")
        start_time = time.time()
        
        model = PeftModel.from_pretrained(
            base_model,
            str(lora_path),
            torch_dtype=torch.float32,    # Use float32 for CPU
            is_trainable=False            # Inference mode only
        )
        
        print(f"‚è±Ô∏è LoRA adapter loaded in {time.time() - start_time:.2f} seconds")
        
        # Test a simple prompt
        print("üîç Testing inference...")
        
        test_prompt = "Human: What are the symptoms of type 2 diabetes and how is it diagnosed?\nAssistant:"
        inputs = tokenizer(test_prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.6,
                top_p=0.9,
                repetition_penalty=1.2,
                no_repeat_ngram_size=2
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = generated_text[len(test_prompt):].strip()
        
        print("\nüìã GENERATED RESPONSE:")
        print("-" * 50)
        print(response)
        print("-" * 50)
        
        print("‚úÖ Model loading and inference test completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error during model testing: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_model_loading()
    if success:
        print("üéâ All tests passed! Your model is working correctly.")
    else:
        print("‚ùå Tests failed. Please check the error messages above.")
